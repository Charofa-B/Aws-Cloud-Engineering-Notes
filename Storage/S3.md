# S3

* Stores objects `(files)` up to `5TB`.
* Object stored with gloabl `unique url` (Universal Namespace)
* Supprts `Object Automatically encrypted` with SSE-S3 (Amazon S3-managed keys) by set `default bucket encryption`.
    * `Decryption occurs automatically upon download`.

<br><br>

## Components
### Bucket
* `Container` for objects.
* `Globally unique name`, but has a region-specific endpoint.
* Defines `access control`, storage charges, and usage reporting.
### Object
* Individual file stored in S3 with `associated metadata`.
* Consists of :
    * `Key` unique `identifier` within a bucket
    * `Value` actual `data`
    * `Metadata` info about the `object` (e.g.,  `content-type`, which helps browsers `render` files `correctly`.)
    * `Version ID` unique version (if versioning enabled)
    * `Sub-resources` e.g., ACLs, lifecycle policies
### Region Specific Storage
* Each `bucket` is `tied` to a specific AWS `Region`.
* Objects `stay` in the selected `Region` `unless` `replicated` or transferred.

<br><br>

## storage classes
### General purpose
* **S3 Standard** For `frequently` accessed data.
* **S3 Intelligent-Tiering** `Moves` objects `automatically` between tiers `based` on `usage`.
* **S3 Express One Zone** `High-performance`, `single-AZ` storage for `short-lived` data.
* **S3 Standard-IA** `Lower-cost`, `infrequent` access, but high `durability`.
  * Store `infrequently accessed` objects while `still needing rapid access` when required.
* **S3 One Zone-IA** `Like IA` but stored in a `single AZ` (lower cost, `lower resilience`).

### Archive
* **S3 Glacier Instant Retrieval** `Low-cost archive`, retrieval in `milliseconds`.
* **S3 Glacier Flexible Retrieval** `Minutes-to-hours retrieval`, `cheaper` than Instant.
* **S3 Glacier Deep Archive** `Lowest cost`, retrieval in `hours`.

### S3 on Outposts
* Brings `S3 storage` `on-premises` with AWS Outposts hardware.

<br><br>

## S3 Policies
* JSON-based rules that define who can access S3 buckets or objects and what actions they can perform.
* Provide fine-grained access control for security and compliance.

### Types
* **Bucket Policy**
  * Applied to a `bucket`.
  * Controls `access` for all `objects` within the `bucket`.
  * Supports `conditions` such as `IP addresses`, `VPC endpoints`, `HTTPS-only` requests.
  * **Example use**: Allow a specific AWS account to read/write objects in your bucket.
* **IAM Policies**
  * Applied to `users`, `groups`, or `roles` in AWS IAM.
  * Grants `permissions` to `perform S3` `actions` across buckets.
  * Example use: Allow a role to upload objects to any S3 bucket in your account.
* **ACL (Access Control List)**
  * `Older`, `simpler` `access` control mechanism.
  * Applied to `individual` `objects` or buckets.
  * Example use: Grant read access to a specific AWS account.
* **S3 Access Points Policies**
  * Applied to `S3 Access Points`, which simplify `managing access to shared datasets`.
  * Example use: Provide a policy for a team that accesses a shared bucket via an access point.
* **S3 Block Public Access**
  * Not a policy `per se`, but a `global setting` to `prevent public access` on buckets or accounts.
  * Useful for `enforcing compliance` and `security`.

<br><br>

## S3 Lifecycle Policies
* Set of `rules` to `automate` the `management` of objects over time.
* Bucket level automation job

### Features
* **Automated Tasks**
  * Move (Transition) objects to cheaper storage classes (Standard → IA → Glacier → Deep Archive).
  * Delete (Expiration) objects automatically.
* **Conditional Application**
  * object age, Object creation date
  * Object tags, Prefixes (folder-like path in bucket)

<br><br>

## Redundancy in S3
1. Upload file to S3
2. S3 `Automatically replicates` file across `multiple AZs` within that region (If We Use 1` AZ` then replication `won t work on cross AZs`)

<br><br>

## Multipart upload
Uploade a `single object` in `multiple parts`, enabling efficient, resilient, and `parallelized` uploads:
* **Independent Uploads** Parts can be `uploaded` in `any order`.
* **Fault Tolerance** `Failed parts` can be `retransmitted` without affecting others.
* **Automatic Assembly** Once all parts are `uploaded`, S3 `combines` them into a `single object`.

*You can Pause and Resume Upload Normally*

<br><br>

## Transfer Acceleration
* `Speeds up` transfers of `data` to S3 buckets from clients `distributed globally`.  
* Useful for applications where users `upload` to a `centralized bucket` from `many regions`.  
* Helps `overcome limitations` of `public internet bandwidth/latency` when sending large data `over long distances`.  

### How It Works
1. Client `uploads data` to the `nearest` Amazon `CloudFront` `edge` location.  
2. The `edge location` uses Amazon’s `optimized AWS` backbone `network` `instead` of the `public internet`.  
3. `Data` is then `securely` and `efficiently` `routed` to the `target S3 bucket` in its AWS Region.  

### Transfer Family
* Fully `managed` AWS `service`
* `Transfer files` into and out of Amazon `S3 storage` or `Amazon Elastic File System (EFS)` over the following `protocols`:
    * Secure Shell (`SSH`) File Transfer Protocol (`SFTP`) version 3
    * File Transfer Protocol Secure (`FTPS`)
    * File Transfer Protocol (`FTP`)
    * Applicability Statement 2 (`AS2`)

**No upfront costs; pay only for what you use.**

<br><br>

## S3 Select
* Allows you to retrieve a `subset of data` from an `object` in S3 using `SQL-like queries`.
* Instead of retrieving the entire object, you can `extract` only the `relevant data`, which `reduces bandwidth`, `latency`, and `cost`.

### Features
* Works with `CSV`, `JSON`, and `Parquet files`.
* Supports `SQL expressions` to `filter rows` and `columns`.
* Can be `combined` with `S3 Glacier` or `S3 Glacier Deep Archive` (with retrieval delay).
* Reduces data `transfer` from `S3` to `applications`.
* Can be integrated with `Athena`, `Redshift`, or `Lambda` for analytics workflows.

### Select Vs [Athena](../Analytics/Athena.md)
* **S3 Select** → retrieves data from a `single object` at a time; essentially returns the subset of `rows/columns you query from that 1 object`.
* [Athena](../Analytics/Athena.md) → retrieves data `across multiple objects` in S3; can `aggregate`, `join`, and `filter` `across large datasets`.
  * Returns `multiple results` from `multiple objects`


<br><br>

## Object Locking
* `Prevents` objects from being `deleted` or `overwritten` for a specified `period` of time
* Designed to help meet `compliance` and `regulatory` requirements

### Key Features:

* **Retention Modes**
    * `Governance Mode` Users with `special permissions` can `override` the lock; `otherwise` objects can’t be deleted or overwritten.
    * `Compliance Mode` `No` user can `delete` or `modify` the object until the retention `period expires`.
* **Retention Period** Define how `long` the object `remains locked` (in days or years).
* **Legal Hold** `Prevent` `deletion` indefinitely, until the `legal hold` is explicitly `removed`.

<br><br>

## S3 versioning
`Protects objects` from accidental `overwrites` and `deletes` by `maintaining` multiple object `versions` on same s3 bucket.


### How It Works
* Creates a `new version` whenever an `object is modified or deleted`.
* `Deleted objects` retain a `delete marker`, but `older versions remain`.

### Bucket-Level Setting
* **Enabled** `Retains` all `versions`, including `delete markers`.
* **Suspended** `Stops versioning` for new objects but `retains` `previous versions`.
* **Disabled (default)** `Overwrites` objects `permanently`.

**Each version increases storage usage, leading to higher costs.**

<br><br>

## Data Replication
`Automatically copies` objects from `one bucket` to `another`, either within the same region (`Same-Region Replication, SRR`) or across regions (`Cross-Region Replication, CRR`).

### How It Works
* `Enable` `replication` on a `source bucket`.
* `Define` the `destination` bucket and the `replication rules` (prefix, tags).
* Every `new object` (or updated object if versioning is enabled) is automatically `copied` to the `destination bucket`.

<br><br>

## S3 Storage Lens
* `Analytics tool` that gives organization-wide visibility into `S3 usage and activity`. 
* It helps you `optimize` cost, improve performance, and ensure data protection.

<br><br>

## S3 Object Lambda
* Allows you to `transform or process S3 objects on-the-fly` when they are `retrieved`.

How it works 
  1. Configure a S3 `Object Lambda` `Access` `Point`.  
  2. When an `object` is `requested`, the request `triggers` a `Lambda function`.  
  3. The `Lambda function` can `modify`, `filter`, or `transform` the `object` data `before returning` it to the client.  

<br><br>

## Logging Requests
Provides detailed `records` for the `requests` that are `made to a bucket`.

<br><br>

## Pricing

### Pay For Only What You Use
* Gigabytes of objects stored (per month) with `different pricing` for `each Region` and `each storage class`
* PUT, COPY, POST, LIST or lifecycle transition to move data into any Amazon S3 storage class

### No Charge For Data Transferred
* Out to the internet for the first 100 GB per month
* In from the internet
* Between S3 buckets or to any service in the same AWS Region
* Out to CloudFront